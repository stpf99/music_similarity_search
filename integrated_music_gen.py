import json
import os
import sys
import argparse
import torch
import librosa
import numpy as np
from pydub import AudioSegment
from audiocraft.models import MusicGen
import soundfile as sf
from sklearn.preprocessing import StandardScaler
import logging
import traceback
import scipy.io.wavfile
from pathlib import Path
import glob
import tempfile
import random

# Wy≈ÇƒÖcz ostrze≈ºenia xFormers i inne
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xFormers.*")
os.environ['XFORMERS_MORE_DETAILS'] = '0'

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('music_gen.log'),
        logging.StreamHandler()
    ]
)

def extract_audio_features(audio_path, sr=22050, duration=30):
    """Ekstraktuj cechy muzyczne z pliku audio u≈ºywajƒÖc librosa"""
    try:
        # Wczytanie audio
        y, sr = librosa.load(audio_path, sr=sr, duration=duration)
        
        # Podstawowe cechy
        features = {}
        
        # Tempo
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
        features['tempo'] = float(tempo.item())  # Extract scalar using .item()
        
        # MFCC (Mel-frequency cepstral coefficients)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        features['mfcc'] = np.mean(mfcc, axis=1).tolist()
        
        # Spectral features
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        features['spectral_centroid'] = float(np.mean(spectral_centroids))
        
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
        features['spectral_rolloff'] = float(np.mean(spectral_rolloff))
        
        zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
        features['zero_crossing_rate'] = float(np.mean(zero_crossing_rate))
        
        # Chroma features (dla tonacji)
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        features['chroma'] = np.mean(chroma, axis=1).tolist()
        
        # Pr√≥ba okre≈õlenia tonacji
        chroma_mean = np.mean(chroma, axis=1)
        key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        key_idx = np.argmax(chroma_mean)
        features['key'] = key_names[key_idx] + ' major'
        
        # RMS Energy
        rms = librosa.feature.rms(y=y)[0]
        features['rms_energy'] = float(np.mean(rms))
        
        # Oszacowanie dancability na podstawie regularno≈õci rytmu
        onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
        onset_times = librosa.frames_to_time(onset_frames, sr=sr)
        if len(onset_times) > 1:
            onset_intervals = np.diff(onset_times)
            rhythm_regularity = 1.0 - np.std(onset_intervals) / np.mean(onset_intervals)
            features['danceability'] = float(max(0, min(1, rhythm_regularity)))
        else:
            features['danceability'] = 0.5
        
        # Energy (na podstawie RMS)
        features['energy'] = float(min(1.0, features['rms_energy'] * 10))
        
        # Valence (na podstawie brightnessu - spectral centroid)
        # Wy≈ºsze czƒôstotliwo≈õci = bardziej pozytywny
        normalized_centroid = min(1.0, features['spectral_centroid'] / 4000.0)
        features['valence'] = float(normalized_centroid)
        
        logging.info(f"Ekstraktowano cechy dla {audio_path}")
        return features
        
    except Exception as e:
        logging.error(f"B≈ÇƒÖd ekstraktowania cech dla {audio_path}: {e}")
        return None

def analyze_audio_directory(directory_path, extensions=None):
    """Analizuj wszystkie pliki audio w katalogu"""
    if extensions is None:
        extensions = ['.mp3', '.wav', '.flac', '.m4a', '.ogg']
    
    directory_path = Path(directory_path)
    if not directory_path.exists():
        logging.error(f"Katalog nie istnieje: {directory_path}")
        return {}
    
    audio_files = []
    for ext in extensions:
        audio_files.extend(directory_path.glob(f'*{ext}'))
        audio_files.extend(directory_path.glob(f'*{ext.upper()}'))
    
    if not audio_files:
        logging.error(f"Nie znaleziono plik√≥w audio w katalogu: {directory_path}")
        return {}
    
    analyzed_files = {}
    print(f"\nüéµ Analizowanie {len(audio_files)} plik√≥w audio...")
    
    for i, audio_file in enumerate(audio_files, 1):
        print(f"üìä Analizowanie {i}/{len(audio_files)}: {audio_file.name}")
        features = extract_audio_features(str(audio_file))
        if features:
            analyzed_files[str(audio_file)] = features
            print(f"   ‚úÖ Tempo: {features['tempo']:.1f} BPM, Tonacja: {features['key']}")
        else:
            print(f"   ‚ùå B≈ÇƒÖd analizy")
    
    return analyzed_files

def find_similar_track(analyzed_files, target_features=None):
    """Znajd≈∫ najbardziej podobny utw√≥r lub wybierz losowy"""
    if not analyzed_files:
        return None, None
    
    if target_features is None:
        # Wybierz losowy utw√≥r
        filepath = random.choice(list(analyzed_files.keys()))
        return filepath, analyzed_files[filepath]
    
    # Znajd≈∫ najbardziej podobny na podstawie cech
    min_distance = float('inf')
    best_match = None
    
    target_mfcc = np.array(target_features['mfcc'])
    
    for filepath, features in analyzed_files.items():
        current_mfcc = np.array(features['mfcc'])
        distance = np.linalg.norm(target_mfcc - current_mfcc)
        
        # Dodaj wagi dla innych cech
        tempo_diff = abs(features['tempo'] - target_features['tempo']) / 200.0
        energy_diff = abs(features['energy'] - target_features['energy'])
        
        total_distance = distance + tempo_diff + energy_diff
        
        if total_distance < min_distance:
            min_distance = total_distance
            best_match = (filepath, features)
    
    return best_match if best_match else (None, None)

def check_dependencies():
    """Sprawd≈∫ czy wszystkie potrzebne zale≈ºno≈õci sƒÖ dostƒôpne"""
    try:
        cuda_available = torch.cuda.is_available()
        logging.info(f"CUDA dostƒôpne: {cuda_available}")
        return True
    except Exception as e:
        logging.error(f"B≈ÇƒÖd podczas sprawdzania zale≈ºno≈õci: {e}")
        return False

def normalize_features(analyzed_files):
    """Normalizuj cechy MFCC"""
    try:
        if not analyzed_files:
            return analyzed_files
            
        mfccs = [features['mfcc'] for features in analyzed_files.values()]
        scaler = StandardScaler()
        normalized_mfccs = scaler.fit_transform(mfccs)
        
        for i, filepath in enumerate(analyzed_files):
            analyzed_files[filepath]['normalized_mfcc'] = normalized_mfccs[i].tolist()
        
        logging.info("Znormalizowano cechy MFCC")
        return analyzed_files
    except Exception as e:
        logging.error(f"B≈ÇƒÖd normalizacji: {e}")
        return analyzed_files

def load_and_preprocess_audio(filepath, duration=10, sr=32000):
    """Za≈Çaduj i przetw√≥rz plik audio"""
    try:
        # Sprawd≈∫ rozszerzenie pliku
        file_ext = Path(filepath).suffix.lower()
        
        if file_ext == '.mp3':
            audio = AudioSegment.from_mp3(filepath)
        elif file_ext == '.wav':
            audio = AudioSegment.from_wav(filepath)
        elif file_ext == '.flac':
            audio = AudioSegment.from_file(filepath, format='flac')
        elif file_ext == '.m4a':
            audio = AudioSegment.from_file(filepath, format='m4a')
        elif file_ext == '.ogg':
            audio = AudioSegment.from_ogg(filepath)
        else:
            audio = AudioSegment.from_file(filepath)
        
        audio = audio[:duration * 1000]
        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(sr)
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)
        samples = samples / 32768.0
        return samples, sr
    except Exception as e:
        logging.error(f"B≈ÇƒÖd przetwarzania audio {filepath}: {e}")
        return None, sr

def load_musicgen_model():
    """Za≈Çaduj model MusicGen z obs≈ÇugƒÖ b≈Çƒôd√≥w"""
    try:
        logging.info("≈Åadowanie modelu MusicGen...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = MusicGen.get_pretrained("facebook/musicgen-small")
        if hasattr(model, 'compression_model'):
            logging.info(f"Model MusicGen za≈Çadowany pomy≈õlnie na {device}")
        else:
            logging.warning("Model MusicGen mo≈ºe nie byƒá w pe≈Çni za≈Çadowany")
        return model, device
    except Exception as e:
        logging.error(f"B≈ÇƒÖd ≈Çadowania modelu: {e}")
        logging.error(traceback.format_exc())
        raise

def create_structured_prompt(features, style_preference=None):
    """Utw√≥rz strukturalny prompt z opisem przebiegu utworu"""
    try:
        key = features.get('key', 'C major')
        tempo = int(features.get('tempo', 120))
        energy = features.get('energy', 0.5)
        danceability = features.get('danceability', 0.5)
        valence = features.get('valence', 0.5)
        
        # Okre≈õl g≈Ç√≥wny styl
        if style_preference:
            main_style = style_preference
        elif danceability > 0.7:
            main_style = "electronic dance"
        elif energy > 0.7:
            main_style = "energetic electronic"
        elif valence < 0.3:
            main_style = "ambient melancholic"
        elif valence > 0.7:
            main_style = "uplifting electronic"
        else:
            main_style = "electronic"
        
        # Okre≈õl strukturƒô utworu
        if tempo >= 120:
            structure = "intro with soft build-up, main section with driving beat, breakdown, climax with full energy, outro with fade"
        elif tempo >= 90:
            structure = "gentle intro, gradual build-up, main melody development, dynamic variation, smooth conclusion"
        else:
            structure = "ambient intro, slow melodic development, harmonic progression, textural evolution, peaceful ending"
        
        # Stw√≥rz pe≈Çny prompt z kontekstem strukturalnym
        structured_prompt = f"{main_style} music in {key} at {tempo} BPM with complete song structure: {structure}"
        
        return structured_prompt
    except Exception as e:
        logging.error(f"B≈ÇƒÖd tworzenia strukturalnego promptu: {e}")
        return f"Electronic music in {features.get('key', 'C major')} at {int(features.get('tempo', 120))} BPM"

def generate_full_track(model, device, prompt, features, duration=30, auto_prompt=False):
    """Generuj pe≈Çny utw√≥r jako jednƒÖ ca≈Ço≈õƒá z naturalnym przebiegiem"""
    try:
        # Przygotuj enhanced prompt
        if auto_prompt or prompt is None:
            enhanced_prompt = create_structured_prompt(features)
        else:
            key = features.get('key', 'C major')
            tempo = int(features.get('tempo', 120))
            
            # Wzbogaƒá prompt o cechy muzyczne
            if 'bpm' not in prompt.lower() and 'tempo' not in prompt.lower():
                enhanced_prompt = f"{prompt}, {tempo} BPM"
            else:
                enhanced_prompt = prompt
                
            if key.lower() not in prompt.lower() and 'key' not in prompt.lower():
                enhanced_prompt = f"{enhanced_prompt} in {key}"
            
            # Dodaj strukturalny kontekst
            enhanced_prompt = f"{enhanced_prompt}, complete song with intro, development, and outro"
        
        logging.info(f"Generowanie pe≈Çnego utworu z promptem: {enhanced_prompt}")
        logging.info(f"Parametry: duration={duration}s, tempo={features.get('tempo')}, key={features.get('key')}")
        
        # Ustaw parametry generowania dla pe≈Çnego utworu
        model.set_generation_params(
            duration=duration,
            use_sampling=True,
            top_k=250,
            top_p=0.9,
            temperature=1.0
        )
        
        print(f"üéµ Generowanie pe≈Çnego utworu ({duration}s)...")
        print("‚è≥ To mo≈ºe potrwaƒá kilka minut...")
        
        # Generuj pe≈Çny utw√≥r jako jednƒÖ ca≈Ço≈õƒá
        with torch.no_grad():
            audio = model.generate([enhanced_prompt], progress=True)
        
        if audio is not None and len(audio) > 0:
            generated_audio = audio[0].cpu().numpy()
            sample_rate = model.sample_rate
            
            # Sprawd≈∫ d≈Çugo≈õƒá wygenerowanego audio
            actual_duration = generated_audio.shape[-1] / sample_rate
            logging.info(f"Wygenerowano pe≈Çny utw√≥r: shape={generated_audio.shape}, duration={actual_duration:.1f}s, sr={sample_rate}")
            
            # Dodaj fade-in i fade-out dla naturalnego brzmienia
            generated_audio = apply_audio_effects(generated_audio, sample_rate)
            
            return generated_audio, sample_rate
        else:
            logging.error("Nie uda≈Ço siƒô wygenerowaƒá utworu")
            return None, None
            
    except Exception as e:
        logging.error(f"B≈ÇƒÖd generowania pe≈Çnego utworu: {e}")
        logging.error(traceback.format_exc())
        return None, None

def apply_audio_effects(audio, sample_rate):
    """Zastosuj efekty audio dla lepszego brzmienia"""
    try:
        # Fade-in (pierwsze 2 sekundy)
        fade_in_samples = int(2.0 * sample_rate)
        if len(audio) > fade_in_samples:
            fade_in_curve = np.linspace(0, 1, fade_in_samples)
            audio[:fade_in_samples] *= fade_in_curve
        
        # Fade-out (ostatnie 3 sekundy)
        fade_out_samples = int(3.0 * sample_rate)
        if len(audio) > fade_out_samples:
            fade_out_curve = np.linspace(1, 0, fade_out_samples)
            audio[-fade_out_samples:] *= fade_out_curve
        
        # Normalizacja g≈Ço≈õno≈õci
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio * (0.95 / max_val)  # Normalizuj do 95% maksymalnej g≈Ço≈õno≈õci
        
        logging.info("Zastosowano efekty audio: fade-in, fade-out, normalizacja")
        return audio
        
    except Exception as e:
        logging.error(f"B≈ÇƒÖd aplikowania efekt√≥w audio: {e}")
        return audio

def extract_mfcc(audio, sr, n_mfcc=13):
    """Wyekstraktuj cechy MFCC z audio"""
    try:
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)
        return np.mean(mfcc, axis=1)
    except Exception as e:
        logging.error(f"B≈ÇƒÖd ekstraktowania MFCC: {e}")
        return None

def create_auto_prompt(features, style_preference=None):
    """Utw√≥rz automatyczny prompt na podstawie cech muzycznych z pe≈ÇnƒÖ strukturƒÖ"""
    try:
        key = features.get('key', 'C major')
        tempo = int(features.get('tempo', 120))
        danceability = features.get('danceability', 0.5)
        energy = features.get('energy', 0.5)
        valence = features.get('valence', 0.5)
        
        # Okre≈õl styl na podstawie cech lub preferencji
        if style_preference:
            style = style_preference
        elif danceability > 0.7:
            style = "dance"
        elif energy > 0.7:
            style = "energetic"
        elif valence < 0.3:
            style = "melancholic"
        elif valence > 0.7:
            style = "happy"
        else:
            style = "balanced"
        
        # Okre≈õl tempo
        if tempo < 90:
            tempo_desc = "slow"
        elif tempo < 120:
            tempo_desc = "medium"
        else:
            tempo_desc = "fast"
        
        energy_level = 'high' if energy > 0.7 else 'medium' if energy > 0.4 else 'low'
        
        # Utw√≥rz prompt z pe≈ÇnƒÖ strukturƒÖ utworu
        auto_prompt = f"{style.capitalize()} electronic music in {key}, {tempo_desc} tempo ({tempo} BPM), {energy_level} energy, complete track with intro, main section, and outro"
        
        print(f"ü§ñ Automatyczny prompt: '{auto_prompt}'")
        return auto_prompt
    except Exception as e:
        logging.error(f"B≈ÇƒÖd tworzenia automatycznego promptu: {e}")
        return "Electronic music with moderate tempo and balanced energy, complete track structure"

def get_user_prompt():
    """Pobierz prompt od u≈ºytkownika interaktywnie lub z argument√≥w"""
    print("\n" + "="*60)
    print("üéµ GENERATOR MUZYKI PODOBNEJ DO TWOJEJ üéµ")
    print("="*60)
    
    parser = argparse.ArgumentParser(description='Generator muzyki podobnej do istniejƒÖcej')
    parser.add_argument('--dir', '-d', type=str, help='Katalog z plikami audio do analizy')
    parser.add_argument('--json', '-j', type=str, help='Plik JSON z wcze≈õniej zapisanymi cechami')
    parser.add_argument('--prompt', '-p', type=str, help='Prompt opisujƒÖcy po≈ºƒÖdanƒÖ muzykƒô')
    parser.add_argument('--duration', type=int, default=30, help='D≈Çugo≈õƒá generowanej muzyki w sekundach (domy≈õlnie 30)')
    parser.add_argument('--style', type=str, help='Preferowany styl muzyczny (electronic, ambient, energetic, dance, etc.)')
    parser.add_argument('--max-duration', type=int, default=120, help='Maksymalna d≈Çugo≈õƒá utworu w sekundach (domy≈õlnie 120)')
    parser.add_argument('--output', '-o', type=str, default='generated_music.wav', help='Nazwa pliku wyj≈õciowego')
    parser.add_argument('--auto', '-a', action='store_true', help='U≈ºyj automatycznego promptu na podstawie cech muzycznych')
    parser.add_argument('--similar', '-s', action='store_true', help='Generuj podobnƒÖ muzykƒô na podstawie losowego utworu z katalogu')
    
    args = parser.parse_args()
    
    return args

def compare_features(generated_audio, sr, original_features):
    """Por√≥wnaj cechy wygenerowanego i oryginalnego audio"""
    try:
        generated_mfcc = extract_mfcc(generated_audio, sr)
        original_mfcc = np.array(original_features['mfcc'])
        if generated_mfcc is not None:
            distance = np.linalg.norm(generated_mfcc - original_mfcc)
            logging.info(f"Odleg≈Ço≈õƒá MFCC miƒôdzy orygina≈Çem a wygenerowanym: {distance:.4f}")
            return distance
        else:
            logging.error("Nie uda≈Ço siƒô wyekstraktowaƒá MFCC z wygenerowanego audio")
            return None
    except Exception as e:
        logging.error(f"B≈ÇƒÖd por√≥wnywania cech: {e}")
        return None

def get_interactive_prompt():
    """Interaktywny interfejs do wyboru promptu"""
    print("\nWybierz spos√≥b generowania muzyki:")
    print("1. üéØ Wprowad≈∫ w≈Çasny prompt (szczeg√≥≈Çowy opis)")
    print("2. ü§ñ Automatyczny prompt na podstawie cech muzycznych")
    print("3. üìã Wybierz z gotowych szablon√≥w")
    print("4. ‚ùå Wyjd≈∫")
    
    while True:
        try:
            choice = input("\nTw√≥j wyb√≥r (1-4): ").strip()
            if choice == '1':
                return get_custom_prompt(), False
            elif choice == '2':
                print("ü§ñ Bƒôdƒô u≈ºywaƒá automatycznego promptu na podstawie analizy twojej muzyki")
                return None, True
            elif choice == '3':
                return get_template_prompt(), False
            elif choice == '4':
                print("üëã Do widzenia!")
                sys.exit(0)
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r. Wybierz 1, 2, 3 lub 4.")
        except KeyboardInterrupt:
            print("\nüëã Do widzenia!")
            sys.exit(0)

def get_custom_prompt():
    """Pobierz niestandardowy prompt od u≈ºytkownika"""
    print("\n" + "-"*50)
    print("üéØ NIESTANDARDOWY PROMPT")
    print("-"*50)
    print("Opisz jakiej muzyki chcesz. Mo≈ºesz u≈ºyƒá:")
    print("‚Ä¢ Gatunek: electronic, rock, jazz, classical, pop")
    print("‚Ä¢ Tempo: slow, medium, fast, lub BPM (np. 120 BPM)")
    print("‚Ä¢ Nastr√≥j: happy, sad, energetic, calm, aggressive")
    print("‚Ä¢ Instrumenty: piano, guitar, drums, synthesizer")
    print("‚Ä¢ Styl: ambient, upbeat, dreamy, powerful")
    
    while True:
        prompt = input("\nüìù Tw√≥j prompt: ").strip()
        if prompt:
            print(f"‚úÖ U≈ºywam promptu: '{prompt}'")
            return prompt
        else:
            print("‚ùå Prompt nie mo≈ºe byƒá pusty. Spr√≥buj ponownie.")

def get_template_prompt():
    """Wybierz prompt z gotowych szablon√≥w"""
    templates = {
        '1': "Upbeat electronic dance music with heavy bass and energetic rhythm",
        '2': "Calm ambient music with soft synthesizers and dreamy atmosphere",
        '3': "Aggressive rock music with electric guitars and powerful drums",
        '4': "Smooth jazz with piano and saxophone, relaxing tempo",
        '5': "Classical orchestral music with strings and piano",
        '6': "Funky house music with groovy bass and rhythmic beats",
        '7': "Chill lo-fi hip hop with soft beats and warm atmosphere",
        '8': "Energetic pop music with catchy melody and upbeat tempo",
        '9': "Dark techno with industrial sounds and driving rhythm",
        '10': "Acoustic folk music with guitar and organic instruments"
    }
    
    print("\nDostƒôpne szablony:")
    for key, value in templates.items():
        print(f"{key:2}. {value}")
    
    while True:
        try:
            choice = input("\nWybierz szablon (1-10): ").strip()
            if choice in templates:
                selected_prompt = templates[choice]
                print(f"‚úÖ Wybra≈Çe≈õ: '{selected_prompt}'")
                return selected_prompt
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r. Wybierz numer od 1 do 10.")
        except KeyboardInterrupt:
            print("\nüëã Do widzenia!")
            sys.exit(0)

def main():
    """G≈Ç√≥wna funkcja programu"""
    try:
        args = get_user_prompt()
        
        if not check_dependencies():
            return
        
        # Okre≈õl ≈∫r√≥d≈Ço danych
        analyzed_files = {}
        
        if args.json:
            # Kompatybilno≈õƒá wsteczna - u≈ºyj istniejƒÖcego JSON
            print(f"üìÅ Wczytywanie danych z pliku JSON: {args.json}")
            with open(args.json, 'r', encoding='utf-8') as f:
                analyzed_files = json.load(f)
        elif args.dir:
            # Analizuj katalog
            print(f"üìÅ Analizowanie katalogu: {args.dir}")
            analyzed_files = analyze_audio_directory(args.dir)
            if not analyzed_files:
                print("‚ùå Nie znaleziono ≈ºadnych plik√≥w audio do analizy")
                return
        else:
            # Interaktywny tryb
            if not args.prompt and not args.auto and not args.similar:
                print("‚ùå Musisz podaƒá --dir z katalogiem audio lub --json z plikiem cech")
                return
        
        if analyzed_files:
            analyzed_files = normalize_features(analyzed_files)
        
        # Okre≈õl prompt i parametry
        user_prompt = args.prompt
        auto_prompt = args.auto
        duration = min(args.duration, args.max_duration)  # Ogranicz maksymalnƒÖ d≈Çugo≈õƒá
        output_file = args.output
        style_preference = args.style
        
        if args.similar:
            # Tryb podobny - wybierz losowy utw√≥r
            if not analyzed_files:
                print("‚ùå Brak danych do analizy podobie≈Ñstwa")
                return
            
            filepath, features = find_similar_track(analyzed_files)
            if filepath and features:
                print(f"üéØ Wybrany utw√≥r bazowy: {Path(filepath).name}")
                print(f"   üìä Tempo: {features['tempo']:.1f} BPM")
                print(f"   üéµ Tonacja: {features['key']}")
                print(f"   ‚ö° Energia: {features['energy']:.2f}")
                auto_prompt = True
                user_prompt = create_structured_prompt(features, style_preference) if not auto_prompt else None
            else:
                print("‚ùå Nie uda≈Ço siƒô wybraƒá utworu bazowego")
                return
        elif not user_prompt and not auto_prompt:
            # Interaktywny tryb wyboru promptu
            user_prompt, auto_prompt = get_interactive_prompt()
            
            if not analyzed_files:
                print("‚ùå Brak danych muzycznych do analizy")
                return
            
            # Wybierz pierwszy dostƒôpny utw√≥r
            filepath = list(analyzed_files.keys())[0]
            features = analyzed_files[filepath]
        else:
            # U≈ºyj pierwszego dostƒôpnego utworu
            if analyzed_files:
                filepath = list(analyzed_files.keys())[0]
                features = analyzed_files[filepath]
            else:
                print("‚ùå Brak danych muzycznych")
                return
        
        logging.info("=== Rozpoczƒôcie generowania muzyki ===")
        logging.info(f"Parametry: prompt='{user_prompt}', duration={duration}s, output='{output_file}', auto={auto_prompt}, style={style_preference}")
        
        # Sprawd≈∫ czy d≈Çugo≈õƒá nie jest zbyt du≈ºa
        if duration > 120:
            print(f"‚ö†Ô∏è  Uwaga: D≈Çugo≈õƒá {duration}s mo≈ºe wymagaƒá du≈ºo czasu i pamiƒôci")
            print("üí° Dla pierwszych test√≥w zalecane jest 30-60 sekund")
        
        # Za≈Çaduj model
        model, device = load_musicgen_model()
        
        # Generuj muzykƒô
        logging.info(f"U≈ºywane cechy: tempo={features.get('tempo')}, key={features.get('key')}")
        
        generated_audio, sample_rate = generate_full_track(
            model, device, user_prompt, features, duration=duration, auto_prompt=auto_prompt
        )
        
        if generated_audio is not None:
            # Przygotuj audio do zapisu
            if len(generated_audio.shape) > 1:
                if generated_audio.shape[0] == 1:
                    audio_to_save = generated_audio.squeeze(0)
                elif generated_audio.shape[1] == 1:
                    audio_to_save = generated_audio.squeeze(1)
                else:
                    audio_to_save = generated_audio[0]
            else:
                audio_to_save = generated_audio
            
            # Zapisz plik
            try:
                sf.write(output_file, audio_to_save, sample_rate)
                logging.info(f"Zapisano wygenerowanƒÖ muzykƒô: {output_file}")
                
                # Weryfikacja
                test_audio, test_sr = sf.read(output_file)
                logging.info(f"Weryfikacja: wczytano {test_audio.shape} przy {test_sr} Hz")
                
                print(f"\n‚úÖ Sukces! Wygenerowana muzyka zosta≈Ça zapisana jako: {output_file}")
                print(f"üéµ D≈Çugo≈õƒá: {len(test_audio)/test_sr:.1f} sekund")
                print(f"üîä Czƒôstotliwo≈õƒá pr√≥bkowania: {test_sr} Hz")
                
                # Por√≥wnaj cechy
                distance = compare_features(audio_to_save, sample_rate, features)
                if distance is not None:
                    similarity = max(0, 100 - distance * 10)
                    print(f"üìä Podobie≈Ñstwo do utworu bazowego: {similarity:.1f}%")
                
            except Exception as save_error:
                logging.error(f"B≈ÇƒÖd zapisywania: {save_error}")
                print(f"‚ùå Nie uda≈Ço siƒô zapisaƒá pliku: {save_error}")
                
        else:
            print("‚ùå Nie uda≈Ço siƒô wygenerowaƒá muzyki")
            
        logging.info("=== Generowanie zako≈Ñczone ===")
        
    except Exception as e:
        logging.error(f"B≈ÇƒÖd krytyczny: {e}")
        logging.error(traceback.format_exc())
        print(f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd krytyczny: {e}")

if __name__ == "__main__":
    main()